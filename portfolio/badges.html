<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head> 
<title>Application: The Badges Problem</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="" />
<meta http-equiv="content-type" content="text/html;charset=utf-8" />
<meta http-equiv="Content-Style-Type" content="text/css" />
<link rel="stylesheet" href="css/blueprint/screen.css" type="text/css" media="screen, projection" />
<link rel="stylesheet" href="css/blueprint/print.css" type="text/css" media="print" />
<link rel="stylesheet" href="css/main.css" type="text/css" media="screen" /> 
<!--[if IE]>
  <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection">
<![endif]-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>
<div class="container">
  <h1>Data Mining Portfolio</h1>
  <p class="author">Dan Baumann</p>

  <h2>Application: The Badges Problem</h2>
  <p class="introduction">Given a data set containing a single string attribute and a binary classification (+ or -), the task was to determine what feature(s) of the string (a person's full name) determine the provided classification.</p>

  <p>The data set consisted of 298 records (213 +, 85 -). An initial manual inspection using regular expressions offered some indication of what features might individually account for the distribution of records among the classifications. For example, a search for /[A-Z]\./ matched the right proportion of records to suggest that abbreviations of names might significantly influence the classification. However this was ruled out by referring to the class labels of the first 100 records that contained abbreviations, which had a similar class split to that of the entire data set. Subsequent inspection in this fashion only served to illuminate the primary flaw in this approach; namely that the class distribution could be the result of multiple dependent features, or worse, multiple independent features. Clearly, a more sophisticated approach would be necessary.</p>

  <p>The possibility of a complex interplay of features suggested that several different modeling approaches would be needed in order to reveal the many different shapes of classification boundaries that may exist in the data. The rectilinear partitioning offered by a decision tree learner such as CART would be sufficient for a simple classification such as "All names containing an abbreviation that have first letter of last name in last half of the alphabet are -, otherwise +". However a more complex classification such as "Names containing at least 5 vowels but no more than 5 letters in the first half of the alphabet are -" might require an oblique decision tree or perhaps a rule-based classifier. The variety of possible feature types suggests a derived data set of high-dimensionality and significant noise; for these reasons the K-nearest neighbor classifier seemed inappropriate.</p>

  <p>Having selected a set classification techniques showing promise for this particular problem, the more formidable challenge of feature creation began. It was decided that these features needed to be orthogonal in nature in order to form the largest possible space in which to describe the records. Knowing that the classification scheme was deliberately generated by a human, the domain was interpreted to be that of English characters as understood by humans. In this domain, some obvious features come to mind: letter frequencies, the length of a word and the parity of that length, and of course the phonetic meaning of the letters (i.e. a vowel or a consonant). Other possible features were also considered, such as word phonetics and semantics; however these seemed too contrived and programming-intensive for this assignment. Based on these considerations, the following 30 basic features were chosen to start the analysis: letter frequency for all 26 characters in the English alphabet, counts of consonants and vowels in each name, and word length for the first and last parts of each name.</p>

  <p>Upon analysis of the initially derived data set with Knime, it was apparent that the word frequencies were dominating the decision making process without contributing much in the way of classification. On the next iteration the word frequencies were removed and parity calculations were added for the vowel and consonant counts, as well as the word lengths. Upon further consideration (and some discussion with colleagues) it was decided that that the letters needed to be considered individually. Therefore, (letter position, letter parity, phonetic class) for the first 6 letters in each name was added to the derived data set. Only the first 6 characters were selected for analysis as this is the length of the shortest name in the data set (with ' ' and '.' characters removed); attempting to analyze more characters would have caused sparsity in the resulting data set and required substitution of missing values.</p>

  <p>The per-character analysis ultimately resulted in an acceptable split on the attribute 'type_1' which is the phonetic type of the second character in each name. Knime's C4.5 decision tree learner indicated that of the 87 names classified as -, 96.6% were correctly classified; additionally, of the 211 names classified as +, 99.5% were correctly classified.</p>

</div>
</body>
</html>